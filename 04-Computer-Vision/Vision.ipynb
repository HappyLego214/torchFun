{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db7e115-1b44-40f7-8938-5dc47676f4e7",
   "metadata": {},
   "source": [
    "**Computer Vision**\n",
    "\n",
    "It is an art of teaching a computer to have eyes. For example you can have model to decide whether or not an image is a cat or a dog - *binary classification*. \n",
    "\n",
    "Or maybe whether the image is of a dog, cat, or chicken - *multiclass classification*. \n",
    "\n",
    "Perhaps checking where an object is in an image? - *object detection*.\n",
    "\n",
    "You can even check where different objects in an image can be seperated from each other *panoptic segmentation*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2185fedb-9fcb-43c4-ae0f-e9b0e6fe06bf",
   "metadata": {},
   "source": [
    "![Display](images/03-computer-vision-problems.png \"Computer Vision Examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b5517a-715e-44ed-8fe6-49c9d6553650",
   "metadata": {},
   "source": [
    "**Applications of Computer Vision**\n",
    "\n",
    "Have a phone? You already used it. Cameras and other photo apps use camera vision in enhancing and sorting images. Modern cars also use computer vision, such as those with Tesla vehicles in avoiding cars, staying in the appropriate lanes, etc. \n",
    "Manufacturers also use computer vision to identify faulty products. \n",
    "\n",
    "Basically, what we use our eyes for is a potential application for computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef83246-14b6-4ce6-98a5-1693a4fa5dfe",
   "metadata": {},
   "source": [
    "**Coverage of Chapter**\n",
    "\n",
    "We're still going to apply all that we've learned before but adding in more 'complexities' each time. But the basic principle always stays the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64da288-8d32-478b-b7e4-42c75e996090",
   "metadata": {},
   "source": [
    "![Display](images/03-pytorch-computer-vision-workflow.png \"How Computer Vision Works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcdf094-6e91-49a3-9dc3-9640a129f2c4",
   "metadata": {},
   "source": [
    "Specifically, we're looking at the following:\n",
    "\n",
    "1. **Computer Vision Libraries in PyTorch** : We'll be going over built-in PyTorch computer vision libraries that can help us in working with images.\n",
    "\n",
    "2. **Load Data** : Before we even get started, we'll always need some form of data. In this chapter we'll be using *FashionMNIST*.\n",
    "\n",
    "3. **Prepare Data** : Once we have the images, we're going to need to prepare them. Just like in cooking. We'll load them into a PyTorch *DataLoader* so that we can use them in a training loop.\n",
    "\n",
    "4. **Model 0 - Building Baseline** : We're working with a multiclass classification model to learn patterns in the data (images). We'll choose a loss function, optimizer, and create a training loop.\n",
    "\n",
    "5. **Making Predictions & Evaluation Model 0** : Use the baseline model, create predictions, and evaluate these predictions.\n",
    "\n",
    "6. **Setup Device Agnostic Code For Future Models** : We're going to have to make iterations of the baseline model so we need to cut some of the work off by shortening how we work with the device.\n",
    "\n",
    "7. **Model 1 - Non-Linearity** : We saw the effects of having no non-linearity before and this time we'll be addding this here to see it's effects again.  We'll see if non-linearity will help improve our baseline model.\n",
    "\n",
    "8. **Model 2 - Convolutional Neural Network (CNN)** : Here we're gonna get into some spicy new field with the introduction of the much powerful convolutional neural network architecture.\n",
    "\n",
    "9. **Comparing Models** : By this point, we've built three different models and we're going to compare them with each other to see what works.\n",
    "\n",
    "10. **Evaluating Best Model** : We'll be creating some predictions on random images and then evaluate the best model.\n",
    "\n",
    "11. **Making Confusion Matrix** : In addition, we'll also be creating a confusion matrix to evaluate a classification model. This will serve as practice and a practical application of how a confusion matrix works and how it helps.\n",
    "\n",
    "12. **Saving & Loading The Chosen Model** : Once we've done everything, we need to start saving our model and try loading it somewhere else so that we can actually work with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a480d-bc8e-4301-904f-24fee07fe02d",
   "metadata": {},
   "source": [
    "**Computer Vision Libraries in PyTorch**\n",
    "\n",
    "We'll be taking a look at at some PyTorch computer libraries that we should be aware of before actually writing code. This will serve as a good jumping off point to making the process a lot less confusing along the line.\n",
    "\n",
    "**torchvision** : this module contains the datasets, model architectures, and image transformations that are vital for computer vision problems. \n",
    "\n",
    "**torchvision.datasets** : in this part, you can find many different datasets for a range of problems that you want to work with from image classification, object detection, image captioning, and even video classifcation + more. This also contains many base classes for making custom datasets that you want to make by yourself. \n",
    "\n",
    "**torchvision.models** : if you want to work with pre-trained models then you can grab one from here. This module contains well-performing and commonly used vision model architectures that are all implemented in PyTorch.\n",
    "\n",
    "**torchvision.transforms** : when working with your dataset, you're going to need to transform them to workable data. Images get turned into numbers, processed, and even augmented. Before working with models, you'll need to work with the data first.\n",
    "\n",
    "**torch.utils.Dataset** : simply the base dataset class for PyTorch.\n",
    "\n",
    "**torch.utils.data.DataLoader** : this creates an iterable over a dataset. Created by calling *torch.utils.data.Dataset*\n",
    "\n",
    "**NOTE:** *torch.utils.Dataset* and *torch.utils.DataLoader* classes are also capable of working with other types of data and not only for computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf8183-8ff2-4ed3-a694-90eb27ec1457",
   "metadata": {},
   "source": [
    "Now that those PyTorch libraries are now explained, it's time to get started by importing the dependencies needed to start working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af082f9-19b8-4b03-9a53-3a42a2e92b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9245a2-35d5-4fe6-a1cf-b4c33b175fbf",
   "metadata": {},
   "source": [
    "**Getting Dataset**\n",
    "\n",
    "Before working with a computer vision problem, we'll need a computer vision dataset! Can't start working without data. So, starting off with FashionMNIST. \n",
    "\n",
    "MNIST stands for Modified National Institute of Standards and Technology \n",
    "\n",
    "The original MNIST dataset consists of thousands of examples of handwritten digits from 0 to 9 - which was in one of the PyTorch tutorials mentioned before. It was used to build computer vision models in identifying postal service numbers. \n",
    "\n",
    "FashionMNIST is in a smiliar setup. Except for the fact that it contains grayscale images of 10 different kinds of clothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec5a80d-8da2-4703-8f7f-4a41ff3c9b06",
   "metadata": {},
   "source": [
    "![Display](images/03-fashion-mnist-slide.png \"What's inside FashionMNSIT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c078e30-84ef-46de-9463-ebddbfc7ffc5",
   "metadata": {},
   "source": [
    "*torchvision.datasets* contains many more examples of datasets that can be used for practicing computer vision code on. FashionMNIST is an example of those datasets. It has 10 different image classes and because of that, it is considered as a multiclassification problem. \n",
    "\n",
    "We'll be creating a computer vision neural network that can identify the different styles of clothing in these images a bit later on. \n",
    "\n",
    "PyTorch has other common computer vision datasets in the *torchvision.datasets*\n",
    "\n",
    "This includes the FasionMNIST in *torchvision.datasets.FashionMNIST()*\n",
    "\n",
    "First, we're going to need to download it and we provide the following parameters:\n",
    "\n",
    "*root: str* - indicating which folder do you want to download to.\n",
    "*train: bool* - this asks whether you want the train (true) or test (false) dataset. \n",
    "*download: bool* - if you want to download the data.\n",
    "*transform: torchvision.transformers* - what would be the transformations to be done with the data.\n",
    "*target_transform* - allowing you to transform the targets (labels) of the data.\n",
    "\n",
    "These parameters are also available with other *torchvision* datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4157a509-7592-46af-8318-91a52141f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Training Data\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # Where to download?\n",
    "    train=True, # Split train / test data?\n",
    "    download=True, # Should download?\n",
    "    transform=ToTensor(), # images are in PIL format - Transform to PyTorch Tensors\n",
    "    target_transform=None # transform labels?\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea83e91-2869-49c2-a529-2ebfc020c295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
       "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
       "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
       "           0.0157, 0.0000, 0.0000, 0.0118],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
       "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0471, 0.0392, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
       "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
       "           0.3020, 0.5098, 0.2824, 0.0588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
       "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
       "           0.5529, 0.3451, 0.6745, 0.2588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
       "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
       "           0.4824, 0.7686, 0.8980, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
       "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
       "           0.8745, 0.9608, 0.6784, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
       "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
       "           0.8627, 0.9529, 0.7922, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
       "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
       "           0.8863, 0.7725, 0.8196, 0.2039],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
       "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
       "           0.9608, 0.4667, 0.6549, 0.2196],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
       "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
       "           0.8510, 0.8196, 0.3608, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
       "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
       "           0.8549, 1.0000, 0.3020, 0.0000],\n",
       "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
       "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
       "           0.8784, 0.9569, 0.6235, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
       "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
       "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
       "           0.9137, 0.9333, 0.8431, 0.0000],\n",
       "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
       "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
       "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
       "           0.8627, 0.9098, 0.9647, 0.0000],\n",
       "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
       "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
       "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
       "           0.8706, 0.8941, 0.8824, 0.0000],\n",
       "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
       "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
       "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
       "           0.8745, 0.8784, 0.8980, 0.1137],\n",
       "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
       "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
       "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
       "           0.8627, 0.8667, 0.9020, 0.2627],\n",
       "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
       "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
       "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
       "           0.7098, 0.8039, 0.8078, 0.4510],\n",
       "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
       "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
       "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
       "           0.6549, 0.6941, 0.8235, 0.3608],\n",
       "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
       "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
       "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
       "           0.7529, 0.8471, 0.6667, 0.0000],\n",
       "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
       "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
       "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
       "           0.3882, 0.2275, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
       "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first training sample\n",
    "image, label = train_data[0]\n",
    "image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cefa3f-dff0-4d6d-99e6-743facac5638",
   "metadata": {},
   "source": [
    "**Input & Output Shapes of a Computer Vision Model**\n",
    "\n",
    "So we have large amount of tensors representing our image which all lead to a single value for the target (label). Let's take a look at the shape of the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002412a5-f9e1-4794-912b-a799bc8e23ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286c097-df23-4874-a8cb-7f664abc4e69",
   "metadata": {},
   "source": [
    "Breaking this down into a simpler explanation. \n",
    "\n",
    "The shape is : [color_channels=1, height=28, width=28]\n",
    "\n",
    "*color_channels=1* signifies that the image is shown in grayscale. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f852447-4f0b-4d19-917f-b503ab922bfc",
   "metadata": {},
   "source": [
    "![Display](images/03-computer-vision-input-and-output-shapes.png \"Image Shapes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0becba-dfcd-49e4-a1c9-bc0bb59aaa1c",
   "metadata": {},
   "source": [
    "Different problems will have different input and output shapes. But the main idea will always stay the same. Turn the data into numbers, create a model to find a pattern in those numbers, and convert those patterns into something meaningful.\n",
    "\n",
    "If *color_channels=3* then the image will be displayed in pixel values of red, green, and blue - RGB. \n",
    "\n",
    "The order of the shape of the tensor that we've seen just now is referred to as *CHW* (Color Channels, Height, Width). \n",
    "\n",
    "However, there are debates on whether an image should be represented as *CHW* which means color channels first or *HWC* which is color channels last.\n",
    "\n",
    "In addition, you will also encounter *NCHW* and *NHWC* formats. *N* just means the number of images. For example, let's say we have a *batch_size=32*. Our tensor shape would reflect that by [32, 1, 28, 28]. We'll go deeper into *batch_sizes* further on.\n",
    "\n",
    "PyTorch's default is usually *NCHW* - channels first for many operators. BUT PyTorch also identifies that *NHWC* - channels last performs better and is considered to be the best practice. \n",
    "\n",
    "Though, in our case, this wouldn't matter as much since we're dealing with a small dataset and the models that we're making are small. \n",
    "\n",
    "But these are some important details to keep in mind in the future.\n",
    "\n",
    "Let's take a look at more shapes of the data that we are working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26b50ebb-38b8-49ef-8a31-90e6536cb67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 60000, 10000, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341db5a-230e-4c00-b95d-4f5804ae81e2",
   "metadata": {},
   "source": [
    "So we are working with 60,000 training samples and 10,000 testing samples. That's quite a jump from the previous work that we were doing and considering the fact that these are images, we're really taking it up a notch. \n",
    "\n",
    "But you'll see that these are pretty much the same thing at the end of the day.\n",
    "\n",
    "Let's take a look at the classes that we have. We can look at them by using the *.classes* attribute with our "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
