{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ee30ac-0f28-4bf1-bf12-4dfe79a5a9c0",
   "metadata": {},
   "source": [
    "**QuickStart**\n",
    "\n",
    "This section goes over the API and the PyTorch library that is most commonly used for machine learning tasks. It is not intended for you to understand the majortiy of the functions and methods being done but for familiarity on what you will encounter going through PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecdbabc-351f-4933-b7bf-72fab968f083",
   "metadata": {},
   "source": [
    "**Working With Data**\n",
    "\n",
    "There are two key fundamental building blocks when it comes to handling data in PyTorch. These are *torch.utils.dataa.DataLoader* and *torch.utils.data.Dataset*. Dataset stores the samples and their associated labels while DataLoader wraps an iterable around the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9c11c50-ea94-4cce-938d-c17f8f054431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922241d2-7bff-41ab-beda-cb0ee378498f",
   "metadata": {},
   "source": [
    "PyTorch offers many convenient libraries to work with datasets. These are TorchText, TorchVision, and TorchAudio. These are libraries on their own that you could use depending on the area that you want to work on. Here, we will be using TorchVision as our dataset is located there. \n",
    "\n",
    "The *torchvision.datasets* module contains Dataset objects for many real-world vision data. Examples are COCO, CIFAR, ImageNet, Flickr8k, etc. Every TorchVision dataset includes two arguments: *transform* and *target_transform*. These are used to modify the samples and labels respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee3292a3-a5d4-425d-b070-00a4c6d69c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to download the training data from datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Used to download the test data from datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# Notice how we specified that for training_data we identified train=True while for our test_data we used train=False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377291a5-79e0-41f7-b86d-f001e5281d04",
   "metadata": {},
   "source": [
    "We pass the Dataset as an argument to DataLoader because DataLoader will use an iterable to wrap over our Dataset. DataLoader also supports automatic batching, sampling, shuffling, and multiprocessing data loading. We specify our batch size as 24. That means each element in the dataloader iterable will return a batch of 64 features and labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a61a55-62b8-4c96-991c-7327a417346a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "#Create data loaders.\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# It is important that both train and test dataloaders have the same batchsize as we will be using these together.\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c911ae5-391e-4e63-bad3-027f425404fd",
   "metadata": {},
   "source": [
    "**Creating Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83cc8ea-c6bb-464e-bd72-bc1f3a2d44fb",
   "metadata": {},
   "source": [
    "Let's try making a model. There'll be a lot of scary stuff involved but once you get to understand the fundamentals in the later chapters, you can come back here and it'll all make sense. So look forward to that. For now, be scared! \n",
    "\n",
    "In order to define a neural network, we need to create a class that inherits from nn.Module. We define the layers of the network in the __init__ functio and specify how data will pass through the network in the forward function. To accelerate oprations in the neural network, we move it to the GPU or MPS if available. \n",
    "\n",
    "This pretty much describes the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62f0688-742a-4571-a061-c9cb93795f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get CPU if CUDA is not available, else use CUDA/GPU\n",
    "a = 10\n",
    "b = 12\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else: \n",
    "    device = \"cpu\"\n",
    "    \n",
    "print(f\"using {device} device\")\n",
    "\n",
    "# Defining a Model\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b8c49-f88d-41f5-9cca-17cebcd063e1",
   "metadata": {},
   "source": [
    "**Optimizing The Model Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3909be1b-8a63-4aa3-ac47-53be46e53c33",
   "metadata": {},
   "source": [
    "Since we've succesfully created a model and specified it's inner-workings. Let's start utilizing the it and initializing the parameters to use in order to train the model succesfully. \n",
    "\n",
    "First, we're going to need to decide on a loss function and an optimizer.  - eg. L1.Loss / SGD\n",
    "\n",
    "In a single training loop, the model will make predictions based on the training dataset in batches that we've specified earlier. Then this backpropagates the prediction error to adjust the model's parameters.\n",
    "\n",
    "Wait, what's backpropagation?! Sounds scary? No. Not really, it's basically just retracing your steps. \n",
    "\n",
    "So there are terms that might be confusing going through machine learning, especially when dealing with models.\n",
    "\n",
    "**Forward Pass:**\n",
    "1. During the forward pass, input data flows through the neural network layers.\n",
    "2. Each layer performs a transformation (such as matrix multiplication, activation function, etc.) on the input.\n",
    "3. The final layer produces the network’s output (predictions).\n",
    "   \n",
    "**Loss Calculation:**\n",
    "1. After obtaining predictions, we compute a loss (error) that quantifies how far off our predictions are from the actual target values.\n",
    "2. The loss function measures the discrepancy between predictions and ground truth.\n",
    "   \n",
    "**Backward Pass (Backpropagation):**\n",
    "1. Here’s where the retracing begins!\n",
    "2. We calculate the gradient of the loss with respect to each model parameter (weights and biases).\n",
    "3. The gradient indicates how much the loss changes when we tweak a parameter slightly.\n",
    "4. We use the chain rule to compute gradients layer by layer, starting from the output layer and moving backward.\n",
    "5. Essentially, we’re figuring out how much each parameter contributed to the overall error.\n",
    "   \n",
    "**Parameter Updates:**\n",
    "7. Armed with gradients, we update the model’s parameters using optimization algorithms (e.g., stochastic gradient descent).\n",
    "8. These updates nudge the parameters in a direction that reduces the loss.\n",
    "9. The process repeats for multiple epochs until convergence.\n",
    "\n",
    "This pretty much describes whats happening inside a model! Still a bit too complicated? Here's an analogy.\n",
    "\n",
    "**Retracing the Steps:**\n",
    "1. Imagine you’re hiking in a dense forest (the neural network).\n",
    "2. You take a path (forward pass) and reach a clearing (predictions).\n",
    "3. Now, you want to find your way back to the trailhead (minimize loss).\n",
    "4. Backpropagation retraces your steps, guiding you through the forest to adjust your route (parameter updates).\n",
    "\n",
    "Given this valuable information. We'll do exactly that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697ffb93-0a25-455c-9304-6d995bd2d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        #Compute Loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch+1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d} {size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf6c420-0cf5-4516-89cf-8352c1df30e8",
   "metadata": {},
   "source": [
    "The function above is for training one epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7c655c-1a43-4ea9-91d4-62f5bdb47a5e",
   "metadata": {},
   "source": [
    "Let's also create a funtion that can test the model's performance with our test dataset to make sure that our model is learning and also learning correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e56beaf-38b4-4a0a-a22b-4721458f35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "111de509-156a-401b-ba8a-3e9d2c0c8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5622bd9-7d16-478c-ad72-c6ac28b5cc74",
   "metadata": {},
   "source": [
    "We have all the functions that we need. We've created our model which was the NeuralNetwork. We also have specified how we can train our model and lastly, we have also created a function to check the accuracy of the training of our model. Now, we just need to repeat this process over and over again. Each iteration is called an epoch! \n",
    "\n",
    "For each epoch, we need to see the accuracy and the loss. So accuracy is the overall effectiveness of our model in our perspective. Our model doesn't actually need the accuracy, it just needs the loss! So basically, accuracy is for human consumption while the weights are for our model's to utilize. It's important that we get both so we can understand how our model is doing and we also get a general overview of it's effectiveness through accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1a76ef-b22e-4cc8-95b3-c953306cf57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: 1\n",
      "---------------------------\n",
      "loss: 2.291268 [   64 60000]\n",
      "loss: 2.284646 [ 6464 60000]\n",
      "loss: 2.270569 [12864 60000]\n",
      "loss: 2.270412 [19264 60000]\n",
      "loss: 2.249402 [25664 60000]\n",
      "loss: 2.230483 [32064 60000]\n",
      "loss: 2.232310 [38464 60000]\n",
      "loss: 2.200326 [44864 60000]\n",
      "loss: 2.202946 [51264 60000]\n",
      "loss: 2.172067 [57664 60000]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 2.163739 \n",
      "\n",
      "Test Error: 2\n",
      "---------------------------\n",
      "loss: 2.172838 [   64 60000]\n",
      "loss: 2.159387 [ 6464 60000]\n",
      "loss: 2.111948 [12864 60000]\n",
      "loss: 2.123531 [19264 60000]\n",
      "loss: 2.068344 [25664 60000]\n",
      "loss: 2.027146 [32064 60000]\n",
      "loss: 2.042439 [38464 60000]\n",
      "loss: 1.966782 [44864 60000]\n",
      "loss: 1.977923 [51264 60000]\n",
      "loss: 1.902617 [57664 60000]\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 1.897839 \n",
      "\n",
      "Test Error: 3\n",
      "---------------------------\n",
      "loss: 1.935545 [   64 60000]\n",
      "loss: 1.898257 [ 6464 60000]\n",
      "loss: 1.792604 [12864 60000]\n",
      "loss: 1.821865 [19264 60000]\n",
      "loss: 1.716495 [25664 60000]\n",
      "loss: 1.677881 [32064 60000]\n",
      "loss: 1.688919 [38464 60000]\n",
      "loss: 1.590014 [44864 60000]\n",
      "loss: 1.621402 [51264 60000]\n",
      "loss: 1.516997 [57664 60000]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.529936 \n",
      "\n",
      "Test Error: 4\n",
      "---------------------------\n",
      "loss: 1.597573 [   64 60000]\n",
      "loss: 1.555252 [ 6464 60000]\n",
      "loss: 1.416020 [12864 60000]\n",
      "loss: 1.481845 [19264 60000]\n",
      "loss: 1.372528 [25664 60000]\n",
      "loss: 1.363528 [32064 60000]\n",
      "loss: 1.372599 [38464 60000]\n",
      "loss: 1.294123 [44864 60000]\n",
      "loss: 1.334585 [51264 60000]\n",
      "loss: 1.239188 [57664 60000]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 1.260883 \n",
      "\n",
      "Test Error: 5\n",
      "---------------------------\n",
      "loss: 1.334501 [   64 60000]\n",
      "loss: 1.309242 [ 6464 60000]\n",
      "loss: 1.156066 [12864 60000]\n",
      "loss: 1.257537 [19264 60000]\n",
      "loss: 1.143733 [25664 60000]\n",
      "loss: 1.159691 [32064 60000]\n",
      "loss: 1.175208 [38464 60000]\n",
      "loss: 1.111686 [44864 60000]\n",
      "loss: 1.155324 [51264 60000]\n",
      "loss: 1.074500 [57664 60000]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.092202 \n",
      "\n",
      "Epochs Completed!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Test Error: {t+1}\\n---------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Epochs Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ca9d9-5050-43eb-9dd1-e12c15d325e2",
   "metadata": {},
   "source": [
    "66% is a respectable number. Let's see if we can do better. Let's do another 5 epochs and see if it improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7ac2996-4e4a-4b64-8f6a-188c42ff06f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: 1\n",
      "---------------------------\n",
      "loss: 1.158862 [   64 60000]\n",
      "loss: 1.152614 [ 6464 60000]\n",
      "loss: 0.985317 [12864 60000]\n",
      "loss: 1.115977 [19264 60000]\n",
      "loss: 1.000349 [25664 60000]\n",
      "loss: 1.022981 [32064 60000]\n",
      "loss: 1.052574 [38464 60000]\n",
      "loss: 0.995721 [44864 60000]\n",
      "loss: 1.038914 [51264 60000]\n",
      "loss: 0.971774 [57664 60000]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.983090 \n",
      "\n",
      "Test Error: 2\n",
      "---------------------------\n",
      "loss: 1.036489 [   64 60000]\n",
      "loss: 1.050346 [ 6464 60000]\n",
      "loss: 0.868631 [12864 60000]\n",
      "loss: 1.022363 [19264 60000]\n",
      "loss: 0.908663 [25664 60000]\n",
      "loss: 0.926903 [32064 60000]\n",
      "loss: 0.972166 [38464 60000]\n",
      "loss: 0.920909 [44864 60000]\n",
      "loss: 0.959006 [51264 60000]\n",
      "loss: 0.903905 [57664 60000]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.909118 \n",
      "\n",
      "Test Error: 3\n",
      "---------------------------\n",
      "loss: 0.946688 [   64 60000]\n",
      "loss: 0.979599 [ 6464 60000]\n",
      "loss: 0.785239 [12864 60000]\n",
      "loss: 0.957573 [19264 60000]\n",
      "loss: 0.847088 [25664 60000]\n",
      "loss: 0.856918 [32064 60000]\n",
      "loss: 0.915952 [38464 60000]\n",
      "loss: 0.871173 [44864 60000]\n",
      "loss: 0.901769 [51264 60000]\n",
      "loss: 0.855819 [57664 60000]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.856354 \n",
      "\n",
      "Test Error: 4\n",
      "---------------------------\n",
      "loss: 0.878156 [   64 60000]\n",
      "loss: 0.926943 [ 6464 60000]\n",
      "loss: 0.723087 [12864 60000]\n",
      "loss: 0.910081 [19264 60000]\n",
      "loss: 0.802916 [25664 60000]\n",
      "loss: 0.804463 [32064 60000]\n",
      "loss: 0.873873 [38464 60000]\n",
      "loss: 0.836661 [44864 60000]\n",
      "loss: 0.859021 [51264 60000]\n",
      "loss: 0.819517 [57664 60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.816629 \n",
      "\n",
      "Test Error: 5\n",
      "---------------------------\n",
      "loss: 0.823972 [   64 60000]\n",
      "loss: 0.884964 [ 6464 60000]\n",
      "loss: 0.674962 [12864 60000]\n",
      "loss: 0.873618 [19264 60000]\n",
      "loss: 0.769369 [25664 60000]\n",
      "loss: 0.764237 [32064 60000]\n",
      "loss: 0.840197 [38464 60000]\n",
      "loss: 0.811164 [44864 60000]\n",
      "loss: 0.825646 [51264 60000]\n",
      "loss: 0.790417 [57664 60000]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.785138 \n",
      "\n",
      "Epochs Completed!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Test Error: {t+1}\\n---------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Epochs Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119274d-e7f9-4691-84c5-f85ed2d8aa34",
   "metadata": {},
   "source": [
    "70% is a lot better! Now that we have our model. Let's try actually using it for ourself. It's a lot more cooler if we can see it in action. But first we have to save it. One common way to save the model is by serializing the internal state dictionary. What does that mean? Well it just means that we save the models parameters (the parameters that we already optimized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d44987d-0363-46ed-a5d8-9fa4a29ac21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State To - model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State To - model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a746e-adea-4867-bd5f-16d76ac52bee",
   "metadata": {},
   "source": [
    "We've saved it, it's time to load it. That means that we are recreating the model's structure and loading the state dictionary (optimized parameters that we just saved) into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23543642-c1a8-4426-b89f-39c85539b8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7179b8c-678d-4526-bd47-86b58c8f7f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax()], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3841030-4863-493e-93a9-6b9dd4cd937e",
   "metadata": {},
   "source": [
    "And that's it for the quickstart. We've created a neural network!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
