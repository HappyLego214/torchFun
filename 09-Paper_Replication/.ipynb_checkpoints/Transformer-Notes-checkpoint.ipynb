{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd06f1e-f206-4c2b-a8af-c122d67e657b",
   "metadata": {},
   "source": [
    "## Visual Guide To Transformer Neural Networks - Video Notes ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d6ceb-3722-4e44-8f7a-43d842f03976",
   "metadata": {},
   "source": [
    "Transformer Architecture - Encoder / Decoder\n",
    "\n",
    "1. Input Data Processing\n",
    "\n",
    "- If training data encompasses the entirety of wikipedia then that means that we will cover all the words in the english language. We assign a numeric index.\n",
    "- What is inputted into the transformer is not the word themselves but their corresponding indices.\n",
    "- Imagine each text index as variable X.\n",
    "\n",
    "Example Text Input -  When | you | play | the | game | of | thrones \n",
    "\n",
    "Example Text Index - [2458,  5670, 234,   987,  398,   607, 1230]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607b73fe-74ce-4208-a149-98a288940242",
   "metadata": {},
   "source": [
    "![Display](notes/vocab-indices.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da59eb26-566c-416f-858b-838d8992e3ff",
   "metadata": {},
   "source": [
    "The inputs are then processed into the next layer - **Embedding Layer**\n",
    "\n",
    "The embedding layer also has indice for every word in the vocabulary, for each of the indices, a vector is attached to it. Initially, these vectors all have random numbers. The model updates these values during the training phase. \n",
    "\n",
    "The size of the vector is a hyperparameter. In the original paper the vector size is 512. To make it easier, we will downscale the vector size into 5 so that they are easier to visualize. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ff3c2-1dcd-44b8-a400-581a765d70e8",
   "metadata": {},
   "source": [
    "![Display](notes/vocab-vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd924fb-df93-4fa3-add0-ffd2d25edbd5",
   "metadata": {},
   "source": [
    "**What is a Word Embedding?**\n",
    "\n",
    "These are vector representations of a given word. Each dimension in the vector represents some kind of linguistic feature. For example, a dimension could be tracking whether it is a verb or not, another could be tracking if it is a noun, etc. If the size of the vector is 512 then that means the word embedding is tracking 512 linguistic features. We can't really identify what information do each of the dimension tracks because that's the work of the neural network. \n",
    "\n",
    "**Graphic Visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e03ed8b-d9ad-4d90-95c9-40181146df95",
   "metadata": {},
   "source": [
    "![Display](notes/word-embedding-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e2b023-8a3d-4e6b-901e-d968ef2b288d",
   "metadata": {},
   "source": [
    "In the graph, we can see how 'close' two words are based on the values that they have on each dimension (denoted as d). Once training starts, words can shift around and get closer or farther from each other. Essentialy words move around the graph and how close they are determines how much two words are associated to each other. So for example, if we have another word 'caterpillar' in the graph. Then surely enough, that would be quite farther as compared to play and game because you won't often find 'caterpillar' used in the same frequency as the other two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f45e5a-73c3-40b6-b4ff-f774d6e02a60",
   "metadata": {},
   "source": [
    "**REMEMBER!** word embeddings are different from input indices.\n",
    "\n",
    "The INPUT EMBEDDINGS LAYER takes in INPUT INDICES and then converts them into WORD EMBEDDINGS.\n",
    "\n",
    "INPUT INDICES -> INPUT EMBEDDINGS LAYER -> WORD EMBEDDINGS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c333f418-3072-4660-af0c-aa7b8c94d83c",
   "metadata": {},
   "source": [
    "**Position Embeddings**\n",
    "\n",
    "Position embeddings are important because they track the sequence of the words. Consider the following: \n",
    "\n",
    "**For a LSTM** - whenever they take in word embeddings, they do this sequentially meaning that each word embedding goes in one at a time. Now this preserves the sequence of the words BUT is slow. They can know which word comes first, second, ... until the last. BUT again it's slow.\n",
    "\n",
    "**For a Transformer** - they take all the words in one go. Now that's considerably faster as compared to LSTMs but of course, this doesn't preserve the sequence / order of the words so the Transformer doesn't know which word goes first, second, .... etc. This poses problems because we all know how important sequence is when it comes to creating sentences. \n",
    "\n",
    "So, with that said and done. How do we make sure that the Transformer can recognize the sequence of the words without making it as slow as an LSTM. That's where *Position Embeddings* start to play a role. We introduce a \"position embedding\" vector that has the same number of dimensions as the \"word embeddings\" vector. \n",
    "\n",
    "We need a way to make these two vectors into one so we just add the position embeddings to the word embeddings. Just like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8611d9e-4df1-44a7-a1d3-373316e505cf",
   "metadata": {},
   "source": [
    "![Display](notes/position-embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c640300-2215-4c4f-95de-c3fd62647719",
   "metadata": {},
   "source": [
    "So pretty simple. We create a new vector for the position embeddings and then add these values to the word embeddings vector. But that leaves out a very important question. What are the values for the position embeddings? There might be a straightforward solution to that to. Just assign 1, 2, 3, 4, ... to the position embeddings. \n",
    "\n",
    "Sadly, it's not that simple. If we use regular integers then once we add the embeddings together then it would drastically distort the values, especially once we get to higher integer positions such as position 30 for example. Adding 0.02 (word embedding) to 30 (position embedding) doesn't sound like it's a good idea.\n",
    "\n",
    "How about fractions? Nope! Won't work either. That's because if we're going to use fractions then we are dependent on the length of the sentence for the values. We should have a consistent value range for each position value regardless of the sentence length. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7116dc-ec8c-439e-bb03-0e27701e2854",
   "metadata": {},
   "source": [
    "![Display](notes/position-values-fraction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2543f5-215d-41ea-912c-ed8ba0ea12bf",
   "metadata": {},
   "source": [
    "So for example, in the image provided above. We can see that \"P1\" for Sentence 1  (0.33) is different from Sentence 2 (0.20). That's because the length of the sentence in Sentence 1 is only up to 3 while Sentence 2 extends to 4. Thus the values are different. That's not a good thing. The values should remain consistent with sentences regardless of sentence length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36200368-6592-4203-a7dc-16d465cc0377",
   "metadata": {},
   "source": [
    "**Wave Frequencies**\n",
    "\n",
    "In the original transformer paper, they managed to solve this 'position' problem by using wave frequencies. Let's take a look at how this is actually done. Use the image below as a reference. \n",
    "\n",
    "\n",
    "* *pos* - stands for the position of the vector\n",
    "* *d* - stands for the number of dimensions in the vector. This should be the same as the number of dimensions in the word embedding vector.\n",
    "* *i* - stands for the indices of the dimensions in the vector.\n",
    "\n",
    "It's easy to get confused with dimensions, indices, and elements. But for a quick rundown here's the definitions:\n",
    "\n",
    "Indices - the means of acccessing the values of a vector.\n",
    "Elements - the values of the vector.\n",
    "Dimensions - the structure of the vector.\n",
    "\n",
    "Consider the vector:\n",
    "\n",
    "[0.1,0.2,0.3,0.4]\n",
    "\n",
    "Elements: The values 0.1, 0.2, 0.3, and 0.4.\n",
    "Indices: The positions 0, 1, 2, and 3 used to access the elements.\n",
    "Dimensions: The vector has 4 dimensions, each representing a different feature or aspect of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c595efea-361c-4045-902e-ba2f922f92b9",
   "metadata": {},
   "source": [
    "![Display](notes/wave-frequencies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd2bd9f-16d7-4336-8ec5-9b0321f8d983",
   "metadata": {},
   "source": [
    "To keep things simple. Imagine using the regular way of integer-based positioning. We increment by the position value so that'll be [1,2,3,4 ...] but instead of applying the position value to each dimension, we instead turn them into frequencies first using the formula above. Now it isn't shown in the picture but we alternate betweeen the sin and cos functions.\n",
    "\n",
    "Why sin and cos? That's because these are frequencies and they are pretty stable. Frequencies will remain that way consistently all throughout so you won't have any problem of values changing with different position lengths. You also avoid shifting position values too much because sin and cos relatively stay between 0 and 1. Lastly, you have many different frequencies available.\n",
    "\n",
    "There are many other diffferent means of getting position values but in the original paper, this is the main one being used so let's focus on that. Let's talk more on the equation itself.\n",
    "\n",
    "One important thing to note is that *i* does not directly mean the position dimension. *i* is the value that you can get from determining if the position dimension is odd or even. More specifically, *i* is the one that determines the frequency. It does not refer to index.\n",
    "\n",
    "It is important to note that one area of confusion is with the term *Dimension* that is involved in calculating *i*. *Dimension* is the index of that specific value in the positional embedding. It is different from *dmodel* that is used in the PE equation. \n",
    "\n",
    "*dmodel* refers to the hyperparameter that is used for the general dimension size of the matrices in the Transformer model.\n",
    "\n",
    "It's important to remember these key differences and while it can be confusing, it is crucial to remember what these different words mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06404063-0871-43fe-93bc-2b8e34186e1f",
   "metadata": {},
   "source": [
    "![Display](notes/solving-for-i.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6ce94-6b90-4634-9a0c-4e3ade93ecdf",
   "metadata": {},
   "source": [
    "![Display](notes/positional-odd-even.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e2d8f-3cce-4b85-ada0-7c9109c0a116",
   "metadata": {},
   "source": [
    "So what is happening is that i = 0 means frequency 0 is assigned to the positional dimensions of 0 and 1. They have the same frequency value but that is alright because 0 uses sine while 1 uses cos so even if they use the same frequency value they still have different values. \n",
    "\n",
    "You can see this being applied all throughout. So the next two positional dimensions would be 2 and 3 and these would also use the same frequency value but still have different values because one would use sine and another would be using cos.\n",
    "\n",
    "Now that we have our positional embeddings, we just add that to our word embeddings then we're done. That's it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15627d-0806-4dde-b406-9ed8ea07e7c8",
   "metadata": {},
   "source": [
    "![Display](notes/positional-and-token.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de09953a-545b-467c-bfe8-eecc84708160",
   "metadata": {},
   "source": [
    "Obviously, we've gotta have something new to call this word + input embedding. No worries, there's a term for this - **Input Embeddings**.\n",
    "\n",
    "Let's recap the process then:\n",
    "\n",
    "INPUT INDICES -> INPUT EMBEDDINGS LAYER -> WORD EMBEDDINGS -> POSITIONAL EMBEDDINGS -> WORD + POS -> INPUT EMBEDDINGS\n",
    "\n",
    "We're pretty much done with the necessary preperations for the inputs and now we can start processing the parts of the Transformer architecture that do a lot of the heavy lifting. We'll now get started working on the **Multi-Head Attention** layer. This part is the big monster that we're gonna have to deal with. You can say that we're getting to the part where the \"magic\" happens.\n",
    "\n",
    "But before we get to explaining the layer - let's ask an important question first. Why is this layer important? What makes it unique? \n",
    "\n",
    "It's because it is able to 'grasp' context. The entire purpose of **Attention** is to emphasize the importance of certain words that drastically impact the meaning of the sentence. For example, 'He managed to fight off a huge beast'. Attention would be able to distinguish the important parts of this sentence such as He, fight, and beast. Attention makes a model understand the important things. \n",
    "\n",
    "However, Transformers aren't innovative because they only utilize attention. Transformers are unique in that they also have another ace up in their sleeve which is called **Self-Attention**.\n",
    "\n",
    "Self attention brings in to the table the ability to understand that a word might mean something entirely different because of the surrounding words that it is with or the 'context' of the word. \n",
    "\n",
    "For example, the word 'model' is different when talking about a fashion model and it is also entirely different when talking about a 3D 'model' not only that but 'model' is also different in the world of machine learning. Attention is able to understand that this single word could be used for entirely different things just basing on the 'context' and paying attention to the surrounding words.\n",
    "\n",
    "So what's the key difference between **Simple-Attention** and **Self-Attention**? Let's take a look through some diagrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f47cc2-a95b-4df4-a5a4-33a17ce5404c",
   "metadata": {},
   "source": [
    "![Display](notes/simple-vs-self-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd6ca9-8ada-4686-8903-ead3133a5203",
   "metadata": {},
   "source": [
    "**Simple-Attention:** This highlights keywords that is most relevant to a specific query. It selectively chooses words and does not take into account other words in the sentence that doesn't exactly answer that query. This process puts more emphasis on words that can answer the query thus posing a challenge because other words that left behind oftentimes are as critical as the focus words in a context-wise sense.\n",
    "\n",
    "**Self-Attention:** On the other hand, self-attention revolves around working with the words in the input. It takes into consideration the relationship between different words thus the values of each word can change depending on the words surrounding it essentially grabbing the 'context'. What's great about self-attention is that it takes all words into account and doesn't fixate on specific words only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729a256-0346-40a3-81d2-60305157d1f9",
   "metadata": {},
   "source": [
    "## Multi-Head Attention ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c2b3bf-2847-4628-b7f8-b0f6cdc116a4",
   "metadata": {},
   "source": [
    "Now, let's take a look where this self-attention mechanism works. We'll be deep diving into the next layer - the multi-head attention layer. Let's start by getting familiar with the contents of the layer and it's innerworkings. See how it is structured and the different parts that it contains. Then we'll start picking these parts off one by one and examining each one. Once we're done, we can start assembling them together back so that we can see how it works entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff50ef59-1332-45be-a5b0-f58981c5c815",
   "metadata": {},
   "source": [
    "![Display](notes/multihead.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c85f8-487a-475f-be35-15afd594fc86",
   "metadata": {},
   "source": [
    "The multihead attention layer is primarily comprised of linear layers. Each having their own seperate weights. The linear layers have different hyperparameters that you can change but for the most part they downscale the input embeddings to save on computation costs. Aside from that they function as regular linear layers of a neural network.\n",
    "\n",
    "Now let's start introducing these three different linear layers: **Key Layer, Query Layer, Value Layer**.\n",
    "\n",
    "The name pretty much explains how these linear layers function. The Query is akin to asking a question - the Key is the answer to the Query - and the Value is what is being returned from the result of the Query-Key.\n",
    "\n",
    "The **Key**, **Query**, and **Value** Linear Layers are composed of a **Weights Matrix** that is randomly initialized which can be updated to be better during training or can be pretrained. \n",
    "\n",
    "So we have the **Key, Query, Value** and inside each is the **Key Weights Matrix, Query Weights Matrix, Value Weights Matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d60e45-0d9d-42f5-b810-660d057fd189",
   "metadata": {},
   "source": [
    "![Display](notes/raw-dimension-inputs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4de409e-241f-4e73-b300-b4e2e77c11f5",
   "metadata": {},
   "source": [
    "Now what do we actually feed these layers? The **Token/Input Embeddings**! Now since we have three of these layers then that means we just duplicate three of the embeddings as well. So each layer gets fed an **Token Embedding**. Of course, we're going to need to transpone the token embeddings first. \n",
    "\n",
    "Once these layers are fed (matrix multiplication) with the token embeddings, they output what we call the actual **Key, Query, Value*** matrices. \n",
    "\n",
    "These are just called **Key, Query, Value**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d020c23-2ce1-485d-aeb3-fd3e86351559",
   "metadata": {},
   "source": [
    "![Display](notes/key-query-value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e55f0a-29d7-4a08-bda7-32edb1b99a55",
   "metadata": {},
   "source": [
    "We'll be locking in first into the **Query-Key** matrices. We have to do a dot product operation between these two. What's a dot product? It calculates how similiar the values are betweeen the Query-Key matrices. This is where we are doing 'self-attention'. One way to visualize this is by imagining an X-dimension where words are placed all over. Certain directions mean something. \n",
    "\n",
    "The word embeddings are then scattered all over this dimension. For example a word 'king' can be close in distance to 'man' because they both mean 'male'. This is what direction means. There is a pretty simple means of explaining the values that result from dot product:\n",
    "\n",
    "* The values are positive if the words are pointing in similiar directions.\n",
    "* The values are zero if the words are pointing two different directions.\n",
    "* The values are negative if the words are pointing in opposite directions.\n",
    "\n",
    "Once we calculate the dot product we get a new matrix of similarity values - or what we call **Attention Scores**. We're not done yet. These attention scores can be very big values or even infinity. We don't want that, we gotta normalize the values. We can do that by simply dividing the results of the dot product by the square root of the key-query dimensions. This process is called **Scaling**. Once we've done this we now have a matrix of **Scaled Scores**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f18a53-cde4-4f26-8eb2-6d3ce06edf0f",
   "metadata": {},
   "source": [
    "![Display](notes/scaling-values.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7453925-88b5-41dc-94c5-ac7bbf80d951",
   "metadata": {},
   "source": [
    "Once we have scaled the values down, we further squash the values to even a smaller number. We need all the values to range between 0 and 1. We do that by using our old trusty softmax function. We're getting into familiar territory now. Not much different when it comes to regular neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb8d44-31d1-4924-bf6c-dc53256df286",
   "metadata": {},
   "source": [
    "![Display](notes/softmax-values.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f2fea-cf48-451d-9a68-5b0d0c456c44",
   "metadata": {},
   "source": [
    "The final output would be something akin to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea040f4e-97c6-41d0-832a-6907476f5c1f",
   "metadata": {},
   "source": [
    "![Display](notes/attention-filter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5444c9-eef3-4fdd-9bd7-5e2cde64a090",
   "metadata": {},
   "source": [
    "The final output is called the **Attention Filter** / **Attention Weights**. The distribution of the weights in the matrix is called the **Attention Pattern**. Notice how all the values in the columns when added up all equal to 1? That's softmax in action. \n",
    "\n",
    "Let's make a pipeline so that we can get a better grasp on the concepts and terms that we've just used: \n",
    "\n",
    "1. INPUT EMBEDDINGS\n",
    "2. KEY & QUERY WEIGHT MATRICES -> KEY & QUERY MATRICES\n",
    "3. DOT PRODUCT OF QUERY AND KEY MATRICES -> ATTENTION SCORE MATRIX\n",
    "4. SCALING -> SCALED SCORES\n",
    "5. APPLY SOFTMAX -> ATTENTION WEIGHTS / ATTENTION FILTER\n",
    "\n",
    "What about the value matrix? The value matrix is pretty simple it goes straight towards the next step so it retains as a value matrix because it's going to be used for something later (we'll get to that). Meanwhile the Query and Key matrices combine to turn into a attention filter. \n",
    "\n",
    "That's quite a lot so quick recap:\n",
    "\n",
    "Our input embeddings go through each linear layer seperately. You can think of it as making three copies of the input embeddings. We pass each of these copies to each layer - Key, Query, Value layers. The value layer outputs a value matrix. \n",
    "\n",
    "The Key and Query layers output a Key and Query matrix where we use the dot product operation to get an attention score matrix. We scale these attention scores to get scaled scores and then further apply the softmax function to finally get the attention weights / attention filter.\n",
    "\n",
    "What are we left with at the end? This: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7458175-a41d-4856-bfbc-e42e76301ae6",
   "metadata": {},
   "source": [
    "![Display](notes/filter-value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8964f886-2571-45f6-bf93-85e654df8306",
   "metadata": {},
   "source": [
    "One thing to keep in mind when we've been discussing this entire process is that there isn't just 'one' attention filter being created. If we were just using one attention filter then this wouldn't be a multi-headed attention layer but instead become a single-headed attention layer. \n",
    "\n",
    "In the original Transformer paper, there were 8 total attention filters being created. GPT-3 has 96 attention heads. Here, we're just going to visualize at 3. Scaling isn't easy when it comes to attention heads because each one involves a lot of parameters and weights. \n",
    "\n",
    "Increasing by one attention head is gonna cost a lot when it comes to computation power so it isn't something that is taken lightly. In addition, there is also the subject of the **Context Size**. The size of the **Attention Matrix** is highly dependent on square of the context size. \n",
    "\n",
    "Confused as to what context size is? That's just another term for the number of tokens that the model takes in per input.\n",
    "\n",
    "This is technically a multiheaded attention layer looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23553bf-962e-482b-92a6-68d33e63dda3",
   "metadata": {},
   "source": [
    "![Display](notes/multi-head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c098e640-4aae-47d4-805f-02cf814ca48a",
   "metadata": {},
   "source": [
    "As you can see, each attention filter works by grabbing certain details from the input. In this case, we're dealing with an image so the attention filters are taking different details from the input. One might be taking the details of Azula, the clouds behind her, or even the mountains. This is why each head of attention is important because they are able to grab more information from the input at the cost of additional computation requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c856c422-34ee-44f5-b8fb-b65687225cf2",
   "metadata": {},
   "source": [
    "![Display](notes/concat-values.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c288d-59dc-42b5-922e-5bb4b4686218",
   "metadata": {},
   "source": [
    "We have all these different attention heads so now what? Each output their own attention filters so what do we do with them? Pretty easy, we just concatenate them into one big matrix. In our specific example, we were using three heads of attention so we'll concat three attention filters. \n",
    "\n",
    "Of course, we want to return the dimensions back to their original size. This is for the reason that we want to keep consistency as much as possible throughout the model. Now the final output is often just called the **Multi-head Attention Output**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43d3a1-2769-42f5-bc23-58c56058fef8",
   "metadata": {},
   "source": [
    "## Add & Normalize ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c203ae5-6e6a-4faa-92dc-04dbacaab7f7",
   "metadata": {},
   "source": [
    "![Display](notes/residiual-connections.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5a5af3-f2af-4f0a-92f0-d61be9c2d643",
   "metadata": {},
   "source": [
    "Time to zoom out a bit and talk about **Residual Connections** - another feature of the Transformer architecture. We know that the multi-head attention layer is important in the sense that it is able to grab important details from the input. In addition, it also allows the input to affect each other. \n",
    "\n",
    "But in this process, we're going through a lot of linear layers which will inevitably overwrite some critical information. This is where residual connections come into play. Residual connections act as a 'highway' wherein the input can go directly skip through certain operations. \n",
    "\n",
    "You can think of this as something that 'reminds' the weights of the previous information. So the token embeddings goes through the multi-head attention layer AND also skips through to the **Add & Norm** layer via a residual connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff9ac4-3950-45ab-a6c2-a9c3c03559e5",
   "metadata": {},
   "source": [
    "Starting off with the simple part of **Adding**. The token embeddings that go through the residual connections is simply added to the output of the multi-head attention layer. The resulting output would then proceed to **Normalization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ebfe3-9df3-4c36-b845-c081894f0c91",
   "metadata": {},
   "source": [
    "![Display](notes/add.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5579c478-0b94-4c59-8b8d-a798ed9cbbd1",
   "metadata": {},
   "source": [
    "Proceeding with normalization, you just get the mean and the standard deviation from each row. Once you have these values then you start going through each neuron/value in the row. The formula to normalize the values is in the image shown below. You can check out the video from the series (episode 3) to see this in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a5bce-476d-49a5-bcd6-f89e3d48d710",
   "metadata": {},
   "source": [
    "![Display](notes/norm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e0baf-e7f2-43e4-9ec5-afcb9545bec9",
   "metadata": {},
   "source": [
    "Then you're done! The resulting output would be the matrix's values are now all normalized and can proceed to the next layer - the **Feed Forward** layer. This step is pretty simple. The Feed Forward layer is composed of linear layers with activation layers in-betweene each. No need to explain much in this case because we've already discussed this before and you should already know this by then.\n",
    "\n",
    "Once you're through with the layer, you go through another add & norm layer as the final layer to finish up. This sums up the entire process of **Encoder** of a Transformer and for the most part, explains the majority of the critical parts of what a Transformer is.\n",
    "\n",
    "Let's start moving on towards the **Decoder** which retains the same concept as the encoder with some slight differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab39bd1-f483-4226-8add-c75ceeb611d5",
   "metadata": {},
   "source": [
    "## Decoder ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d3cf22-d7de-4b46-b310-344e829123fc",
   "metadata": {},
   "source": [
    "As compared to the encoder, which takes just one input—the base input text—the decoder takes in two: the output of the encoder and the generated text thus far. There is quite a slight difference in where these inputs to the decoder come in as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3fd490-2956-41a6-84d6-5b29219192bf",
   "metadata": {},
   "source": [
    "![Display](notes/decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b9978-fab1-4df3-b34b-e9f652ac8359",
   "metadata": {},
   "source": [
    "There are a couple of things to dissect in the image of how this Transformer works. First, we duplicate the output of the encoder to create two instances that act as **Key and Value** inputs for the Multi-Head Attention layer of the decoder. The **Query Matrix**, however, comes from the decoder's output embeddings. But what do we input to the output embeddings layer?\n",
    "\n",
    "You might ask, \"But this is the first pass, so the decoder hasn't generated any text yet. What goes through the output embeddings?\"\n",
    "\n",
    "Great question! The initial input for the output embeddings is a special token called < Start >. It goes through the embedding process: gets turned into a token, receives a positional embedding, and then goes through the decoder’s multi-head attention layer (ignore the masking part for now; we'll get back to that). It then passes through an Add & Norm layer and a feed-forward network, eventually becoming the Query matrix for the encoder-decoder multi-attention layer. This Query matrix goes quite a long way before converging with the Key and Value inputs from the encoder.\n",
    "\n",
    "The special multi-head attention layer where the encoder and decoder converge is called the **Encoder-Decoder Multiattention Layer**.\n",
    "\n",
    "So, summing it all up: the multi-head attention layer of the decoder takes in the Key and Value matrices from the encoder. The decoder produces the Query matrix from the embeddings of the generated text. Initially, if it hasn't yet generated any text, the first thing that goes through the output embeddings of the decoder is the special token <Start>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ed223-d06e-4aaf-b0eb-24db804dd871",
   "metadata": {},
   "source": [
    "Now, the Encoder-Decoder Multiattention Layer is pretty much the same as before so there's no need to explain much aside from the fact that it gets the Key-Value matrices from the Encoder and the Query matrix from the Output Embeddings of the Decoder. \n",
    "\n",
    "Aside from that, the processs is pretty straightforward. The layer outputs a matrix that is fed to a Feed Forward and Add & Norm layer. Finally, it goes through a Linear layer. We'll focus in on this one because it's important since it's the last layer but expect it to be as the same as how a regular Linear layer operates.\n",
    "\n",
    "This last linear layer serves as a 'classifier' because this is the part where it predicts what is the next word to come out. The linear layer's outputs is dependent on the number of classes that we have. For example, if we were classifying between a dog or a cat, then there would be two outputs for the layer. In our case, it's going to be the entire vocabulary list that we have. \n",
    "\n",
    "Now what is exactly being fed to this classifier? As it stands now, what we have is the output matrix from the previous Add & Norm layer. If we just send this directly into the linear layer, it'll output vectors. We don't want that. What we want are scalar values for every word. That way we can pick the word which has the maximum score. \n",
    "\n",
    "How do we do that? First, we flatten the entire matrix into one single row. We concatenate these and then start passing them onto the linear layer. Because of this the output of the classifier will now be a score for each value. These are **Logits**. Of course, we don't want to work with just logits. We have to convert these again with softmax so that we can have a probability distribution.\n",
    "\n",
    "With that done, we're ready to pick the next word to generate. The one with the biggest possibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43233e4c-0bfe-425a-a3b3-8f5cc4881b01",
   "metadata": {},
   "source": [
    "![Display](notes/linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d308fc5c-8943-4c85-8dec-74387a87d5ea",
   "metadata": {},
   "source": [
    "!!!!! **THIS ENTIRE SEGMENT IS NOT YET VERIFIABLE** !!!!!!\n",
    "\n",
    "--------------------\n",
    "\n",
    "**SEGWAY:** Normally flattening isn't applied during inference. Only used in training because it facilitates with loss calculation. Some loss functions want the data to be in a specific shape so that is why we need to flatten the data. \n",
    "\n",
    "Recall - Each token contains the length of a vocabulary size. The matrix that the Add & Norm layer would output is akin to something like [batch_size, seq_length, hidden_dim]. When it is fed through the linear layer - it would become [batch_size, seq_length, vocab_size]. Wherein vocab_size is the length of each token in the seq_length. Remember that seq_length contains vectors of logits. The batch_size contains the number of sequences. So each batch_size would contain a differet number of sequences in seq_length. \n",
    "\n",
    "In flattening, we concatenate the seq_length with batch_size. After flattening this would become [batch_size * seq_length, vocab_size]. This is now a vector that contains the number of tokens that we have and across all batches. This is similiar to how we multiply the Height & Width of pixels in a CNN. In our case, we are working with tokens (words) and the number of batches we have. \n",
    "\n",
    "**NOTE:** With vocab_size, this is literally just a number that indicates the number of unique tokens (words, subwords, or characters) that the model can predict. It is not like Width in CNNs wherein inside are pixel values. There is nothing inside vocab_size. The batch_size on the other hands contains a lot of different sequences contained in seq_length. So each batch size contains different sequences.\n",
    "\n",
    "**NOTE:** Make sure to remember that * in this case is not multiplication but concatenation. With flattening, we are NOT changing values just RESHAPING the way it is presented. WE NEVER TOUCH THE VALUES NOR CHANGE IT IN FLATTENING.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61245816-16d7-4e17-bcce-f63e50c25b9c",
   "metadata": {},
   "source": [
    "!!!!! **THIS ENTIRE SEGMENT IS NOT YET VERIFIABLE** !!!!!!\n",
    "\n",
    "--------------------\n",
    "\n",
    "**FLATTENING IS NOT FOR INFERENCE:**\n",
    "\n",
    "Flattening makes inference much more complicated because it loses the positional structure of the input. When working with [batch_siz, seq_length, vocab_size], We can identify the last token easily because it is the last element in seq_length. So we just choose the highest value in the last vector of seq_length as the next word. \n",
    "\n",
    "For example - we are working with [1,3,100]. We narrow down to last element of seq_length and just choose from the 100 elements inside. \n",
    "\n",
    "If we do flattening then it'll be much harder. Because the seq_length is concatenated with vocab size. So we need to know exactly where the borders are between each token. So if we do flattening we will instead have [1,300]. This layout is easier when training because we have truth values to base on and calculate the loss directly 1-1. That's not the case with inference.\n",
    "\n",
    "We just made it harder for ourselves to predict the next word because we lost the positional structure of where the token begins and ends.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c1f16-b919-4a49-a9e4-60a8f83d9070",
   "metadata": {},
   "source": [
    "Once we have started predicting the first word from the decoder - the cycle restarts again. The new genereated word alongside the previous words (in this case < start> ) is passed onto the output embeddings of the decoder. It get's a positional embedding and goes through all the layers in the decoder until the next word is generated. This process repeats until we get to the end where the decoder generates a special < end > token that indicates that everything is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd77d8c-58ce-48cb-80d2-4ef2b5f7bf21",
   "metadata": {},
   "source": [
    "## Masking in Multi-Head Attention ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8423f1e8-3797-48b5-b876-e9e50a9138c9",
   "metadata": {},
   "source": [
    "Masking is applied during the training phase of a transformer model. There are key differences between inference and training. During training, we are able to directly calculate the loss function and improve our model because we have truth labels to compare to. However, for inference we're creating words on our own thus we're not directly sure if what we're predicting is correct. \n",
    "\n",
    "We just hope that it's correct based on the loss and accuracy metrics that we have during training. There is no guarantee that during inference, what we're predicting is correct. During training, we have a set of inputs such as *\"when life gives you lemons\"* then we have a truth label that is what should the model predict *\"make lemonades\"*. Simply put we have questions and we have answers. This allows the model to learn from it's mistakes. \n",
    "\n",
    "Now how does this play in masking? Technically speaking, we already have the answers. But we have to make sure that the model doesn't cheat. So we need to find a way to hide the answer from the model. That is where masking comes to play. \n",
    "\n",
    "Let's assume that the decoder already has generated it's first word - \"fight\". Rather than sending in the newly generated word into the decoder, the next input would be the first word of the masked truth labels. We unmask \"make\" and send it to the decoder. This way the decoder can calculate the loss between it's prediction and the truth which can be used to improve the weights and make the model better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf78c0a-dba9-47cc-af7a-592a64d73829",
   "metadata": {},
   "source": [
    "![Display](notes/mask-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498cfa21-547a-47e7-9814-2773375471aa",
   "metadata": {},
   "source": [
    "**Deeper Into The Attention Layer**\n",
    "\n",
    "Recall back to how the attention layer work. Before we proceed to the softmax operation of the scaled scores of the attention matrix, we apply masking first. So masking works by basically setting all the future tokens into negative infinity. What this usually looks like is that it halves the entire the attention matrix diagonally. Wherein the upper right half is set to negative infinity while the bottom left half contains values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548876f0-4b3b-44c4-8796-8580b6644399",
   "metadata": {},
   "source": [
    "![Display](notes/masking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bad254c-9ce5-48ae-8aad-c2d37b4e2350",
   "metadata": {},
   "source": [
    "Notice how in the image shown that future words are set to negative infinity. Starting at \"I\" - it can only see itself but anything further is set to negative infinity. That repeats again til the < end > mark. \n",
    "\n",
    "Once you've applied the softmax function it would look much better with the attention values only working with the previous words that have been already generated. \n",
    "\n",
    "Again, you can't take into context words that still haven't been generated. Just akin to human speech. You can't consider words that haven't yet been spoken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46467f90-0b28-4b3a-8536-479177ca6c35",
   "metadata": {},
   "source": [
    "![Display](notes/masking-filter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d3901-8e93-4e21-ae5d-de1b92fdeace",
   "metadata": {},
   "source": [
    "Let's take another look at this in a different perspective. You can see here the process of how attention works during training with masking. At the start, it has access to the full input from the encoder so it includes them in attention. Also notice that the < start> of the decoder is the < end > of the encoder. The \"I\" would be just unmasked from the decoder then this will be included in the next iteration. This process keeps repeating over and over until < end > is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e464a4-5b25-4786-b66b-a812e3c6af10",
   "metadata": {},
   "source": [
    "![Display](notes/attention-start.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd49cf8f-c712-4703-b8ea-61c67f5367f6",
   "metadata": {},
   "source": [
    "![Display](notes/attention-end.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f97d2-18b6-4b8e-9f3b-58bc734d2fe8",
   "metadata": {},
   "source": [
    "So that pretty much sums up how Transformers work! Now this is without talking about certain specifics such as batching but that'll be much more apparent when working with code. Right now, we're focused on explaining the big parts of the Transformer architecture. Not to mention that there are many different methods of how this actually works basing on the many iterations of the architecture. But for the most part, this is how the original paper works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af03aa0-85eb-4364-a558-405d26fd69b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
